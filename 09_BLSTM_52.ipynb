{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lVnOsc134QxJTD4_snHHYg8X8uULtlAZ",
      "authorship_tag": "ABX9TyNRRAOMRO2RpCa+nJMvbo6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramanp83/Processing-Transcript_file/blob/main/09_BLSTM_52.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install contractions"
      ],
      "metadata": {
        "id": "xz08QgCjTZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAwxjO1zRtim"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions  # Import contractions\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input\n",
        "from tensorflow.keras.layers import Masking\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "# !pip install nltk==3.8.1\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt_tab') # Download punkt_tab\n",
        "\n",
        "# Define stopwords and contraction mappings\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''format for K's directory'''\n",
        "# def load_transcripts(data_dir):\n",
        "#     transcripts = []\n",
        "#     target_folders = [f\"{i}_P\" for i in range(300, 305)]  # Folders to process\n",
        "#     target_file = \"300_TRANSCRIPT.csv\"  # File to read within each folder\n",
        "\n",
        "#     for folder_name in target_folders:  # e.g. 300_P,etc\n",
        "#         folder_path = os.path.join(data_dir, folder_name)  # concatenate\n",
        "#         # e.g. data_dir + folder_name = full path i.e. till 300_P\n",
        "#         # Check if the target folder exists\n",
        "#         if os.path.isdir(folder_path):  # if folder_path exists\n",
        "#             file_path = os.path.join(folder_path, target_file)\n",
        "\n",
        "#             # Check if the target file exists within the folder\n",
        "#             if os.path.isfile(file_path):  # give full path to 300_Transcript.csv\n",
        "#                 df = pd.read_csv(file_path, sep='\\t', names=['start', 'end', 'speaker', 'value'])\n",
        "#                 # Add Participant_ID column\n",
        "#                 participant_id = int(folder_name.split('_')[0])\n",
        "#                 df['Participant_ID'] = participant_id\n",
        "#                 transcripts.append(df)\n",
        "#             else:\n",
        "#                 print(f\"File '{target_file}' not found in folder '{folder_name}'\")\n",
        "#         else:\n",
        "#             print(f\"Folder '{folder_name}' not found in '{data_dir}'\")\n",
        "\n",
        "#     return transcripts\n",
        "\n",
        "\n",
        "# transcripts = load_transcripts(\"/content/drive/MyDrive/Transcipts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wvkwOu-mSWgr",
        "outputId": "9ebe2cf4-307d-4628-86a7-0c6af8094726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"format for K's directory\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_transcripts(data_dir):\n",
        "    transcripts = []\n",
        "    # Target files (300 to 352, excluding 342 and 344)\n",
        "    target_files = [f\"{i}_TRANSCRIPT.csv\" for i in range(300, 311) if i not in [342, 344]]\n",
        "\n",
        "    for file_name in target_files:\n",
        "        file_path = os.path.join(data_dir, file_name)\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "            df = pd.read_csv(file_path, sep='\\t', names=['start', 'end', 'speaker', 'value'])\n",
        "            # Add Participant_ID column\n",
        "            participant_id = int(file_name.split('_')[0])\n",
        "            df['Participant_ID'] = participant_id\n",
        "            transcripts.append(df)\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found in folder '{data_dir}'\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "# Call the function with the updated directory\n",
        "transcripts = load_transcripts(\"/content/drive/MyDrive/datasset/transcript\")"
      ],
      "metadata": {
        "id": "AC3o_2gvUfZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = {\n",
        "    \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\", \"i'm\": \"i am\",\n",
        "    \"you're\": \"you are\", \"it's\": \"it is\", \"that's\": \"that is\", \"let's\": \"let us\",\n",
        "    \"aren't\": \"are not\", \"isn't\": \"is not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
        "    \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\", \"didn't\": \"did not\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
        "    \"shouldn't\": \"should not\", \"couldn't\": \"could not\", \"mightn't\": \"might not\",\n",
        "    \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she's\": \"she is\", \"he's\": \"he is\",\n",
        "    \"we're\": \"we are\", \"they're\": \"they are\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
        "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\",\n",
        "    \"i'd\": \"i would\", \"you'd\": \"you would\", \"he'd\": \"he would\", \"she'd\": \"she would\",\n",
        "    \"we'd\": \"we would\", \"they'd\": \"they would\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "    \"what's\": \"what is\", \"who's\": \"who is\", \"how's\": \"how is\", \"where's\": \"where is\",\n",
        "    \"why's\": \"why is\", \"let's\": \"let us\", \"y'all\": \"you all\", \"d'you\": \"do you\",\n",
        "    \"gonna\": \"going to\", \"gotta\": \"got to\", \"wanna\": \"want to\", \"c'mon\": \"come on\",\n",
        "    \"ain't\": \"is not\", \"o'clock\": \"of the clock\", \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\", \"should've\": \"should have\", \"would've\": \"would have\",\n",
        "    \"might've\": \"might have\", \"must've\": \"must have\", \"you've\": \"you have\",\n",
        "    \"we've\": \"we have\", \"they've\": \"they have\", \"i've\": \"i have\",\n",
        "    \"it'd\": \"it would\", \"there'd\": \"there would\", \"here'd\": \"here would\",\n",
        "    \"that'd\": \"that would\", \"this'd\": \"this would\", \"who'd\": \"who would\",\n",
        "    \"what'd\": \"what did\", \"where'd\": \"where did\", \"how'd\": \"how did\"\n",
        "}"
      ],
      "metadata": {
        "id": "sNs95fWfUztL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions_manual(text):\n",
        "    for contraction, full_form in contractions_dict.items():\n",
        "        text = text.replace(contraction, full_form)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "def clean_text_enhanced(text):\n",
        "    # Convert to string if not already, handling NaN and other types\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove incomplete word annotations\n",
        "    text = text.replace(\"XXX\", \"[UNK]\")  # Replace unrecognizable words\n",
        "    text = expand_contractions_manual(text)  # Expand contractions manually\n",
        "    text = remove_stopwords(text)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply enhanced cleaning to all transcripts\n",
        "for df in transcripts:\n",
        "    if 'value' in df.columns:\n",
        "        df['processed_text'] = df['value'].apply(clean_text_enhanced)\n",
        "    else:\n",
        "        print(\"Column 'value' not found in DataFrame.\")\n",
        "\n",
        "print(\"Transcript 0th index is are :\", transcripts[0].head(1))\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "\n",
        "# Load Universal Sentence Encoder\n",
        "embed_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "def generate_embeddings(text_list):\n",
        "    embeddings = embed_model(text_list)\n",
        "    print(\"Shape of embeddings:\", embeddings.shape)  # Check shape\n",
        "    return np.array(embeddings)\n",
        "\n",
        "MAX_TIMESTEPS = 400\n",
        "EMBEDDING_DIM = 512\n",
        "\n",
        "# Generate embeddings for processed text\n",
        "for df in transcripts:\n",
        "    if 'processed_text' in df.columns:\n",
        "        all_embeddings = generate_embeddings(df['processed_text'].tolist())\n",
        "\n",
        "        # Reshape to (num_samples, embedding_dim) if necessary\n",
        "        all_embeddings = all_embeddings.reshape(-1, EMBEDDING_DIM)\n",
        "\n",
        "        df['embeddings'] = all_embeddings.tolist()# Make sure embeddings are stored as lists of 512-dimensional vectors\n",
        "         # Convert to list of lists\n",
        "    else:\n",
        "        print(\"Column 'processed_text' not found in DataFrame.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02XnRICbVibU",
        "outputId": "d775f0fd-f12b-4a8f-968c-80d5a05da9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript 0th index is are :         start        end  speaker  value  Participant_ID processed_text\n",
            "0  start_time  stop_time  speaker  value             300          value\n",
            "Shape of embeddings: (175, 512)\n",
            "Shape of embeddings: (182, 512)\n",
            "Shape of embeddings: (187, 512)\n",
            "Shape of embeddings: (192, 512)\n",
            "Shape of embeddings: (205, 512)\n",
            "Shape of embeddings: (406, 512)\n",
            "Shape of embeddings: (214, 512)\n",
            "Shape of embeddings: (278, 512)\n",
            "Shape of embeddings: (225, 512)\n",
            "Shape of embeddings: (177, 512)\n",
            "Shape of embeddings: (241, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences_uniform(embeddings):\n",
        "       # Reshape if needed (avoid unnecessary copy if already 3D)\n",
        "       if embeddings.ndim == 2:\n",
        "           embeddings = embeddings.reshape(embeddings.shape[0], 1, EMBEDDING_DIM)\n",
        "\n",
        "       # Pad sequences (use in-place modification if possible)\n",
        "       padded_embeddings = pad_sequences(embeddings, maxlen=MAX_TIMESTEPS,\n",
        "                                        dtype='float32', padding='post',\n",
        "                                        truncating='post')\n",
        "       return padded_embeddings\n",
        "\n",
        "\n",
        "# Merge metadata with transcript data\n",
        "train_metadata = pd.read_csv('/content/drive/MyDrive/Transcipts/train_split_Depression_AVEC2017.csv')\n",
        "dev_metadata = pd.read_csv('/content/drive/MyDrive/Transcipts/dev_split_Depression_AVEC2017.csv')\n",
        "test_metadata = pd.read_csv('/content/drive/MyDrive/Transcipts/test_split_Depression_AVEC2017.csv')\n",
        "\n",
        "# Concatenate all transcripts\n",
        "all_transcripts = pd.concat(transcripts)"
      ],
      "metadata": {
        "id": "bQnocVgFV1g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge metadata with transcript data based on Participant_ID\n",
        "merged_data = pd.merge(train_metadata, all_transcripts, on='Participant_ID', how='inner')\n",
        "participants_with_transcripts = all_transcripts['Participant_ID'].unique()\n",
        "merged_data = merged_data[merged_data['Participant_ID'].isin(participants_with_transcripts)]\n"
      ],
      "metadata": {
        "id": "4lTC_tLPWlBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Iterate through each participant group\n",
        "# for participant_id, group_data in participant_groups:\n",
        "#     # Get all embeddings for the participant\n",
        "#     participant_embeddings = group_data['embeddings'].tolist()\n",
        "\n",
        "#     # Pad the embeddings for the participant\n",
        "#     padded_participant_embeddings = pad_sequences_uniform(participant_embeddings)\n",
        "\n",
        "#     #append all padded embedding to a general list\n",
        "#     all_padded_embeddings.append(padded_participant_embeddings)\n",
        "#NO ERROR"
      ],
      "metadata": {
        "id": "ZUn1hkGIWshw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "participant_groups = merged_data.groupby('Participant_ID')\n",
        "\n",
        "# Initialize all_padded_embeddings as a list\n",
        "all_padded_embeddings = []\n",
        "\n",
        "\n",
        "# Iterate through each participant group and pad embeddings\n",
        "for participant_id, group_data in participant_groups:\n",
        "    participant_embeddings = group_data['embeddings'].tolist()\n",
        "    participant_embeddings = np.array(participant_embeddings)\n",
        "\n",
        "    # Use pad_sequences_uniform for padding and reshaping\n",
        "    padded_participant_embeddings = pad_sequences_uniform(participant_embeddings)\n",
        "\n",
        "     # Extend all_padded_embeddings with padded embeddings for this participant\n",
        "    all_padded_embeddings.extend(padded_participant_embeddings)\n",
        "\n",
        "\"\"\"Explanation of Changes:\n",
        "\n",
        "Initialize as a list: We now initialize all_padded_embeddings as an empty list [] instead of a NumPy array.\n",
        "Append to the list: Inside the loop, we use all_padded_embeddings.append(padded_participant_embeddings) to add the padded embeddings for each participant to the list.\n",
        "Concatenate into an array: After the loop, we use np.concatenate(all_padded_embeddings, axis=0) to combine all the padded embeddings from the list into a single NumPy array.\n",
        "\"\"\"\n",
        "# Convert to list of lists for DataFrame assignment\n",
        "all_padded_embeddings = [emb.tolist() for emb in all_padded_embeddings]\n",
        "\n",
        "# Assign the list of lists to the DataFrame column\n",
        "merged_data['padded_embeddings'] = all_padded_embeddings\n",
        "#ValueError: cannot reshape array of size 76800 into shape (192,400,512)\n"
      ],
      "metadata": {
        "id": "cNjxwTliYFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Due to this error\n",
        "\"\"\"ValueError                                Traceback (most recent call last)\n",
        "<ipython-input-67-3dd57b6f7d51> in <cell line: 3>()\n",
        "      2 X_train_reshaped = []\n",
        "      3 for x in X_train:\n",
        "----> 4   X_train_reshaped.append(np.array(x).reshape(MAX_TIMESTEPS, EMBEDDING_DIM))\n",
        "      5 X_train = np.array(X_train_reshaped)\n",
        "      6\n",
        "\n",
        "ValueError: cannot reshape array of size 400 into shape (400,512)\"\"\"\n",
        "#PROBELM WITH ABOVE METHOD OF APPEND\n",
        "\"\"\"The Problem:\n",
        "\n",
        "The core issue is that you are trying to directly assign a large NumPy array (all_padded_embeddings) as values to a column in a pandas DataFrame (merged_data['padded_embeddings']). This can be inefficient and may not always work as intended.\n",
        "\n",
        "Why This Happens:\n",
        "\n",
        "Pandas DataFrames are designed to store data in a tabular format, where each column typically contains values of a consistent data type. When you try to assign a large NumPy array to a single column, pandas might encounter difficulties in handling the complex data structure.\n",
        "\n",
        "Suggested Solution:\n",
        "\n",
        "Instead of directly assigning the NumPy array, consider converting it to a list of lists, where each inner list represents the padded embeddings for a particular row in the DataFrame. This way, pandas can manage the data more effectively. \"\"\"\n",
        "\n",
        "\"\"\"participant_groups = merged_data.groupby('Participant_ID')\n",
        "\n",
        "all_padded_embeddings = []\n",
        "# Iterate through each participant group\n",
        "for participant_id, group_data in participant_groups:\n",
        "    # Get all embeddings for the participant\n",
        "    participant_embeddings = group_data['embeddings'].tolist()\n",
        "\n",
        "    # Pad the embeddings for the participant\n",
        "    # Before padding, make sure each element has the correct dimensionality\n",
        "    padded_participant_embeddings = pad_sequences(\n",
        "        [np.array(emb) for emb in participant_embeddings],  # Ensure each element is a numpy array\n",
        "        maxlen=MAX_TIMESTEPS,\n",
        "        dtype='float32',\n",
        "        padding='post',\n",
        "        truncating='post'\n",
        "    )\n",
        "\n",
        "    # Extend all_padded_embeddings with the padded embeddings for this participant\n",
        "    all_padded_embeddings.extend(padded_participant_embeddings)\n",
        "\n",
        "# Convert the list of padded embeddings to a list of lists\n",
        "all_padded_embeddings = [emb.tolist() for emb in all_padded_embeddings]  # Convert to list of lists\n",
        "\n",
        "# Now all_padded_embeddings should have the correct shape\n",
        "merged_data['padded_embeddings'] = all_padded_embeddings  # Assign list of lists to DataFrame column\"\"\""
      ],
      "metadata": {
        "id": "dq-Lcv9oqC6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "1aede0f6-fc9a-4005-deb6-798f17cfcc9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"participant_groups = merged_data.groupby('Participant_ID')\\n\\nall_padded_embeddings = []\\n# Iterate through each participant group\\nfor participant_id, group_data in participant_groups:\\n    # Get all embeddings for the participant\\n    participant_embeddings = group_data['embeddings'].tolist()\\n\\n    # Pad the embeddings for the participant\\n    # Before padding, make sure each element has the correct dimensionality\\n    padded_participant_embeddings = pad_sequences(\\n        [np.array(emb) for emb in participant_embeddings],  # Ensure each element is a numpy array\\n        maxlen=MAX_TIMESTEPS,\\n        dtype='float32',\\n        padding='post',\\n        truncating='post'\\n    )\\n\\n    # Extend all_padded_embeddings with the padded embeddings for this participant\\n    all_padded_embeddings.extend(padded_participant_embeddings)\\n\\n# Convert the list of padded embeddings to a list of lists\\nall_padded_embeddings = [emb.tolist() for emb in all_padded_embeddings]  # Convert to list of lists\\n\\n# Now all_padded_embeddings should have the correct shape\\nmerged_data['padded_embeddings'] = all_padded_embeddings  # Assign list of lists to DataFrame column\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Transform the list into a single array\n",
        "# all_padded_embeddings = np.concatenate(all_padded_embeddings, axis=0)\n",
        "# # Reshape to (samples, timesteps, embedding_dim)\n",
        "\n",
        "# # Get the total number of samples\n",
        "# num_samples = all_padded_embeddings.shape[0] // MAX_TIMESTEPS\n",
        "\n",
        "# # Reshape considering the embedding dimension\n",
        "# all_padded_embeddings = all_padded_embeddings.reshape(num_samples, MAX_TIMESTEPS, EMBEDDING_DIM)\n",
        "#Error :ValueError: cannot reshape array of size 2854000 into shape (7135,400,512)\n",
        "\n",
        "\n",
        "# merged_data['padded_embeddings'] = list(all_padded_embeddings)"
      ],
      "metadata": {
        "id": "osqiMNnhW1i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing PHQ8_Score if necessary (optional)\n",
        "merged_data = merged_data.dropna(subset=['PHQ8_Score'])\n",
        "\n",
        "# Fill missing values in numeric columns with the column's mean\n",
        "numeric_columns = merged_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "merged_data[numeric_columns] = merged_data[numeric_columns].fillna(merged_data[numeric_columns].mean())\n",
        "# Reset index\n",
        "merged_data.reset_index(drop=True, inplace=True)  # Reset the index after dropping rows\n"
      ],
      "metadata": {
        "id": "pp9Y3S_ZW2tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "# Split the data\n",
        "X = np.stack(merged_data['padded_embeddings'].values)  # Now we transform the values to an array of arrays\n",
        "y = merged_data['PHQ8_Binary']\n",
        "# Check shape of X before the split\n",
        "print(\"Shape of X before the split:\", X.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "# Reshape X_train and X_val to ensure correct dimensionality\n",
        "X_train = X_train.reshape(-1, MAX_TIMESTEPS, EMBEDDING_DIM)\n",
        "X_val = X_val.reshape(-1, MAX_TIMESTEPS, EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "MVOOGaTQcmEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the type and shape of data\n",
        "print(f\"X_train type: {type(X_train)}, shape: {np.array(X_train).shape}\")\n",
        "print(f\"X_val type: {type(X_val)}, shape: {np.array(X_val).shape}\")\n",
        "print(f\"y_train type: {type(y_train)}, shape: {np.array(y_train).shape}\")\n",
        "print(f\"y_val type: {type(y_val)}, shape: {np.array(y_val).shape}\")\n"
      ],
      "metadata": {
        "id": "ElXiZCY6cqFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #why no need they are already\n",
        "# # Ensure X_train, X_val are numpy arrays\n",
        "# X_train = np.array(X_train)\n",
        "# X_val = np.array(X_val)"
      ],
      "metadata": {
        "id": "wD3vTlidcx11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure y_train, y_val are numpy arrays and the correct shape\n",
        "y_train = np.array(y_train).reshape(-1, 1)  # Reshape if needed for binary classification\n",
        "y_val = np.array(y_val).reshape(-1, 1)  # Reshape if needed for binary classification"
      ],
      "metadata": {
        "id": "54gu0b8Uc-lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "DqdtdSuAdCCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## # Define input shape for the LSTM layer\n",
        "# ## input_shape = (MAX_TIMESTEPS, EMBEDDING_DIM)  # (400, 512)\n",
        "\n",
        "# ## # Define the input layer\n",
        "# ## inputs = Input(shape=input_shape)\n",
        "\n",
        "# # Define the input layer with the input shape\n",
        "# inputs = Input(shape=(MAX_TIMESTEPS, EMBEDDING_DIM)) # input shape\n",
        "\n",
        "# # Now, instead of input_shape in the first layer, you'll pass 'inputs' to avoid the only warning\n",
        "# # Define the model layers, starting with the Bidirectional LSTM layer\n",
        "# model = Sequential([\n",
        "#     ### inputs, # Add the input layer\n",
        "#     Bidirectional(LSTM(200, return_sequences=True))(inputs),  # Pass inputs to the LSTM layer\n",
        "#     # Bidirectional(LSTM(200, return_sequences=True, input_shape=(MAX_TIMESTEPS, EMBEDDING_DIM))),\n",
        "#     Bidirectional(LSTM(200)),\n",
        "#     Dense(500, activation='relu'),\n",
        "#     Dense(100, activation='relu'),\n",
        "#     Dense(60, activation='relu'),\n",
        "#     Dense(1, activation='sigmoid')  # For binary classification\n",
        "# ])\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# \"\"\"#Warning\n",
        "# /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
        "#   super().__init__(**kwargs)\"\"\"\n",
        "\n",
        "# \"\"\"#Warning2\n",
        "# Okay, I understand that you are still encountering the same warning,even after implementing\n",
        "#      the previous fix.I think I see the issue. Despite defining an Input layer, it's added as a\n",
        "#      layer within the Sequential model instead of being passed to the first layer.\"\"\"\n"
      ],
      "metadata": {
        "id": "wz9l17lpdTqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape for the LSTM layer\n",
        "input_shape = (MAX_TIMESTEPS, EMBEDDING_DIM)  # (400, 512)\n",
        "\n",
        "# Define the input layer\n",
        "inputs = Input(shape=input_shape)\n",
        "# Now, instead of input_shape in the first layer, you'll pass 'inputs' to avoid the only warning\n",
        "# Define the model layers, starting with the Bidirectional LSTM layer\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True), input_shape=input_shape))\n",
        "model.add(Bidirectional(LSTM(200)))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QhR37unjhBop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the padded embeddings within each row of X_train and X_val\n",
        "X_train_reshaped = []\n",
        "for x in X_train:\n",
        "  X_train_reshaped.append(np.array(x).reshape(MAX_TIMESTEPS, EMBEDDING_DIM))\n",
        "X_train = np.array(X_train_reshaped)\n",
        "\n",
        "X_val_reshaped = []\n",
        "for x in X_val:\n",
        "  X_val_reshaped.append(np.array(x).reshape(MAX_TIMESTEPS, EMBEDDING_DIM))\n",
        "X_val = np.array(X_val_reshaped)\n",
        "\n",
        "# Convert your NumPy arrays to TensorFlow tensors\n",
        "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "X_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_tensor, y_train_tensor, validation_data=(X_val_tensor, y_val_tensor), epochs=20, batch_size=32)\n"
      ],
      "metadata": {
        "id": "ewTUrHVlcUp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X_train and X_val to include the embedding dimension\n",
        "X_train = X_train.reshape(-1, MAX_TIMESTEPS, EMBEDDING_DIM)\n",
        "X_val = X_val.reshape(-1, MAX_TIMESTEPS, EMBEDDING_DIM)\n",
        "\n",
        "# Convert your NumPy arrays to TensorFlow tensors\n",
        "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "X_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_tensor, y_train_tensor, validation_data=(X_val_tensor, y_val_tensor), epochs=20, batch_size=32)\n"
      ],
      "metadata": {
        "id": "FkU54rH9mmxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9oISFINoiWlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "class TranscriptProcessor:\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # Load Universal Sentence Encoder\n",
        "        self.encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    def load_transcripts(self, start_id: int = 300, end_id: int = 352, exclude_ids: List[int] = [342, 344]) -> pd.DataFrame:\n",
        "        \"\"\"Load and combine transcript files\"\"\"\n",
        "        all_transcripts = []\n",
        "        for session_id in range(start_id, end_id + 1):\n",
        "            if session_id in exclude_ids:\n",
        "                continue\n",
        "            file_path = self.data_dir / f\"{session_id}_TRANSCRIPT.csv\"\n",
        "            try:\n",
        "                transcript = pd.read_csv(file_path, sep='\\t')\n",
        "                transcript['Participant_ID'] = session_id\n",
        "                all_transcripts.append(transcript)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Error loading transcript {session_id}: {e}\")\n",
        "        return pd.concat(all_transcripts, ignore_index=True)\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if isinstance(text, float):\n",
        "            text = str(text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [t for t in tokens if t not in self.stopwords]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings using Universal Sentence Encoder\"\"\"\n",
        "        # Process in batches to handle memory constraints\n",
        "        batch_size = 32\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = self.encoder(batch_texts)\n",
        "            embeddings.append(batch_embeddings.numpy())\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "class DepressionDataset(Dataset):\n",
        "    def __init__(self, embeddings: np.ndarray, labels: np.ndarray, device: torch.device):\n",
        "        self.embeddings = torch.FloatTensor(embeddings).to(device)\n",
        "        self.labels = torch.FloatTensor(labels).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    \"\"\"LSTM with attention mechanism for depression detection\"\"\"\n",
        "    def __init__(self, input_size: int = 512, hidden_size: int = 200, num_layers: int = 2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.MultiheadAttention(hidden_size * 2, num_heads=8)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add sequence dimension for attention if needed\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
        "        pooled = torch.mean(attn_out, dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "class DepressionDetectionPipeline:\n",
        "    def __init__(self, data_dir: str):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.processor = TranscriptProcessor(data_dir)\n",
        "\n",
        "    def prepare_data(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare data for model training\"\"\"\n",
        "        transcripts = self.processor.load_transcripts()\n",
        "        metadata = pd.read_csv(self.processor.data_dir / '/content/drive/MyDrive/datasset/transcript/train_split_Depression_AVEC2017 (1).csv')\n",
        "        merged_data = transcripts.merge(metadata, on='Participant_ID')\n",
        "        merged_data['cleaned_text'] = merged_data['value'].apply(self.processor.preprocess_text)\n",
        "        embeddings = self.processor.generate_embeddings(merged_data['cleaned_text'].tolist())\n",
        "        labels = merged_data['PHQ8_Binary'].values\n",
        "        return embeddings, labels\n",
        "\n",
        "    def train_model(self, embeddings: np.ndarray, labels: np.ndarray,\n",
        "                   hidden_size: int = 200, batch_size: int = 32,\n",
        "                   num_epochs: int = 20) -> Tuple[AttentionLSTM, Dict]:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(embeddings, labels, test_size=0.2)\n",
        "\n",
        "        train_dataset = DepressionDataset(X_train, y_train, self.device)\n",
        "        val_dataset = DepressionDataset(X_val, y_val, self.device)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        model = AttentionLSTM(input_size=512, hidden_size=hidden_size).to(self.device)\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        metrics_history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training loop implementation remains the same...\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs.squeeze(), batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation loop implementation remains the same...\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_preds = []\n",
        "            val_true = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    outputs = model(batch_x)\n",
        "                    val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
        "                    val_preds.extend(outputs.squeeze().cpu().numpy())\n",
        "                    val_true.extend(batch_y.cpu().numpy())\n",
        "\n",
        "            val_f1 = f1_score(val_true, np.round(val_preds))\n",
        "\n",
        "            metrics_history['train_loss'].append(train_loss / len(train_loader))\n",
        "            metrics_history['val_loss'].append(val_loss / len(val_loader))\n",
        "            metrics_history['val_f1'].append(val_f1)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "            print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "            print(f\"Val F1: {val_f1:.4f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "        return model, metrics_history\n",
        "\n",
        "def main():\n",
        "    pipeline = DepressionDetectionPipeline('/content/drive/MyDrive/datasset/transcript')\n",
        "    embeddings, labels = pipeline.prepare_data()\n",
        "    model, metrics = pipeline.train_model(embeddings, labels)\n",
        "    pd.DataFrame(metrics).to_csv('training_metrics.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dWMk4vAbjE-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c475dfb9-7a43-42f6-d90c-53d87ebd35c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.6591\n",
            "Val Loss: 0.6398\n",
            "Val F1: 0.0000\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6575\n",
            "Val Loss: 0.6412\n",
            "Val F1: 0.0000\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6557\n",
            "Val Loss: 0.6436\n",
            "Val F1: 0.0000\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6563\n",
            "Val Loss: 0.6392\n",
            "Val F1: 0.0000\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6563\n",
            "Val Loss: 0.6402\n",
            "Val F1: 0.0000\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6572\n",
            "Val Loss: 0.6449\n",
            "Val F1: 0.0000\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6576\n",
            "Val Loss: 0.6423\n",
            "Val F1: 0.0000\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6552\n",
            "Val Loss: 0.6391\n",
            "Val F1: 0.0000\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6551\n",
            "Val Loss: 0.6481\n",
            "Val F1: 0.0000\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6565\n",
            "Val Loss: 0.6395\n",
            "Val F1: 0.0000\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6556\n",
            "Val Loss: 0.6414\n",
            "Val F1: 0.0000\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6554\n",
            "Val Loss: 0.6398\n",
            "Val F1: 0.0000\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6555\n",
            "Val Loss: 0.6419\n",
            "Val F1: 0.0000\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6552\n",
            "Val Loss: 0.6405\n",
            "Val F1: 0.0000\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6572\n",
            "Val Loss: 0.6424\n",
            "Val F1: 0.0000\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6568\n",
            "Val Loss: 0.6399\n",
            "Val F1: 0.0000\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6558\n",
            "Val Loss: 0.6435\n",
            "Val F1: 0.0000\n",
            "Epoch 18/20\n",
            "Train Loss: 0.6562\n",
            "Val Loss: 0.6412\n",
            "Val F1: 0.0000\n",
            "Epoch 19/20\n",
            "Train Loss: 0.6555\n",
            "Val Loss: 0.6432\n",
            "Val F1: 0.0000\n",
            "Epoch 20/20\n",
            "Train Loss: 0.6559\n",
            "Val Loss: 0.6428\n",
            "Val F1: 0.0000\n"
          ]
        }
      ]
    }
  ]
}